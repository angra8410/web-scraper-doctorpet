# Web Scraper para DoctorPet.co - Categor√≠a Alimentos

> üéØ **Orientado a desarrolladores junior** - Este proyecto incluye documentaci√≥n detallada y explicaciones paso a paso para facilitar el aprendizaje.

## üìã Descripci√≥n

Este es un web scraper en Python dise√±ado para extraer informaci√≥n de productos de la categor√≠a "Alimentos" del sitio web [DoctorPet.co](https://doctorpet.co/producto-category/alimentos/).

### ¬øQu√© es un web scraper?

Un web scraper es un programa que autom√°ticamente visita p√°ginas web y extrae informaci√≥n espec√≠fica de ellas. Piensa en √©l como un "robot lector" que puede:
- Abrir p√°ginas web
- Leer su contenido HTML
- Buscar informaci√≥n espec√≠fica (como nombres de productos, precios, etc.)
- Guardar esa informaci√≥n en un formato estructurado (como CSV)

## ‚ú® Caracter√≠sticas

El scraper extrae la siguiente informaci√≥n de cada producto:
- ‚úÖ **Nombre del producto**
- ‚úÖ **Precio** (incluyendo precios en oferta)
- ‚úÖ **Disponibilidad** (Disponible/Agotado)
- ‚úÖ **Enlace** al producto
- ‚úÖ **Imagen** del producto

Adem√°s, incluye:
- üîÑ **Manejo autom√°tico de paginaci√≥n** - Recorre todas las p√°ginas de la categor√≠a
- ‚è±Ô∏è **Delays entre peticiones** - Evita sobrecargar el servidor y ser bloqueado
- üîÅ **Reintentos autom√°ticos** - Si una petici√≥n falla, lo intenta de nuevo
- üìä **Exportaci√≥n a CSV** - Resultados en formato f√°cil de abrir en Excel
- üìù **Logging detallado** - Muestra el progreso y ayuda a detectar problemas

## üéì Patrones de Dise√±o Aplicados

Este proyecto implementa dos patrones importantes para facilitar el onboarding y comprensi√≥n:

### 1. Chain of Thought Pattern (Cadena de Pensamiento)

Cada secci√≥n del c√≥digo incluye comentarios detallados explicando **por qu√©** se tom√≥ cada decisi√≥n t√©cnica, no solo **qu√©** hace el c√≥digo.

**Ejemplo:**
```python
# Chain of Thought: Usamos Session en lugar de requests.get() directo porque:
# - Reutiliza conexiones (m√°s eficiente)
# - Mantiene cookies autom√°ticamente
# - Permite configurar comportamiento com√∫n para todas las peticiones
self.session = requests.Session()
```

Este patr√≥n ayuda a:
- Entender el razonamiento detr√°s de las decisiones
- Aprender mejores pr√°cticas
- Facilitar futuras modificaciones

### 2. Persona Pattern (Orientado a Desarrollador Junior)

Toda la documentaci√≥n y comentarios est√°n escritos asumiendo que el lector es un **desarrollador junior** que puede estar aprendiendo:
- Python
- Web scraping
- Peticiones HTTP
- Manejo de archivos CSV

**Ejemplo:**
```python
# Nota para junior: CSV = Comma Separated Values (Valores Separados por Comas)
# Es un formato universal, f√°cil de abrir en Excel/Google Sheets
```

## üöÄ Instalaci√≥n

### Prerrequisitos

- Python 3.7 o superior instalado en tu sistema
- pip (gestor de paquetes de Python)

### Paso 1: Clonar el repositorio

```bash
git clone https://github.com/angra8410/web-scraper-doctorpet.git
cd web-scraper-doctorpet
```

### Paso 2: Instalar dependencias

```bash
pip install -r requirements.txt
```

**¬øQu√© se instalar√°?**
- `requests` - Para hacer peticiones HTTP (visitar p√°ginas web)
- `beautifulsoup4` - Para analizar y extraer datos del HTML
- `lxml` - Parser r√°pido para BeautifulSoup

## üìñ Uso

### Uso b√°sico

Simplemente ejecuta el script:

```bash
python scraper.py
```

El scraper:
1. Visitar√° la p√°gina de alimentos de DoctorPet.co
2. Extraer√° informaci√≥n de todos los productos
3. Navegar√° por todas las p√°ginas disponibles
4. Guardar√° los resultados en un archivo CSV con nombre autom√°tico como: `doctorpet_alimentos_20240930_143025.csv`

### Uso avanzado

Si quieres usar el scraper desde otro script Python:

```python
from scraper import DoctorPetScraper

# Crear instancia del scraper
scraper = DoctorPetScraper()

# Scrapear solo las primeras 2 p√°ginas (√∫til para testing)
productos = scraper.scrape_category(max_pages=2)

# Guardar en un archivo espec√≠fico
scraper.save_to_csv(productos, filename="mis_productos.csv")
```

## üìÇ Estructura de Archivos

```
web-scraper-doctorpet/
‚îÇ
‚îú‚îÄ‚îÄ scraper.py           # Script principal del scraper
‚îú‚îÄ‚îÄ requirements.txt     # Dependencias del proyecto
‚îú‚îÄ‚îÄ README.md           # Este archivo
‚îú‚îÄ‚îÄ .gitignore          # Archivos a ignorar en git
‚îÇ
‚îî‚îÄ‚îÄ doctorpet_alimentos_*.csv  # Archivos CSV generados (ignorados por git)
```

## üìä Formato del CSV

El archivo CSV generado tiene las siguientes columnas:

| nombre | precio | disponibilidad | enlace | imagen |
|--------|--------|----------------|--------|--------|
| Alimento Perro Adulto 15kg | $45.000 | Disponible | https://... | https://... |
| Alimento Gato Cachorro 3kg | $28.000 | Agotado | https://... | https://... |

Puedes abrir este archivo con:
- Microsoft Excel
- Google Sheets
- LibreOffice Calc
- Cualquier editor de texto

## üîß Modificaci√≥n del Scraper

### Cambiar la categor√≠a a scrapear

Modifica la constante `BASE_URL` en `scraper.py`:

```python
# En lugar de alimentos, scrapear otra categor√≠a
BASE_URL = "https://doctorpet.co/producto-category/juguetes/"
```

### Ajustar los delays (pausas)

Si el sitio te est√° bloqueando, aumenta los delays:

```python
DELAY_BETWEEN_REQUESTS = 5  # En lugar de 2 segundos
DELAY_BETWEEN_PRODUCTS = 1  # En lugar de 0.5 segundos
```

**¬øPor qu√© son importantes los delays?**
- Evitan sobrecargar el servidor del sitio web
- Previenen que tu IP sea bloqueada
- Son una pr√°ctica √©tica de web scraping

### Extraer informaci√≥n adicional

Si quieres extraer m√°s datos (por ejemplo, categor√≠as, SKU, etc.), modifica el m√©todo `_extract_product_info()`:

```python
def _extract_product_info(self, product_element):
    product_data = {
        'nombre': 'N/A',
        'precio': 'N/A',
        # ... campos existentes ...
        'categoria': 'N/A',  # Nuevo campo
    }
    
    # Extraer categor√≠a
    category_element = product_element.find('span', class_='product-category')
    if category_element:
        product_data['categoria'] = category_element.get_text(strip=True)
    
    # ... resto del c√≥digo ...
```

No olvides actualizar tambi√©n el m√©todo `save_to_csv()`:

```python
fieldnames = ['nombre', 'precio', 'disponibilidad', 'enlace', 'imagen', 'categoria']
```

## üêõ Troubleshooting (Soluci√≥n de Problemas)

### Problema 1: "No se encontraron productos"

**Posibles causas:**
1. El sitio web cambi√≥ su estructura HTML
2. El sitio est√° bloqueando el scraper
3. Problemas de conectividad

**Soluciones:**
1. Inspecciona manualmente la p√°gina web:
   - Abre https://doctorpet.co/producto-category/alimentos/ en tu navegador
   - Click derecho > "Inspeccionar elemento"
   - Verifica que los productos tengan `class="product"`
   
2. Aumenta los delays entre peticiones
3. Verifica tu conexi√≥n a internet

### Problema 2: "Error de conexi√≥n" o "Timeout"

**Posibles causas:**
- Problemas de red
- El sitio web est√° ca√≠do
- Tu IP puede estar bloqueada

**Soluciones:**
1. Verifica que puedes acceder al sitio desde tu navegador
2. Espera unos minutos y vuelve a intentar
3. Aumenta el `REQUEST_TIMEOUT`:
   ```python
   REQUEST_TIMEOUT = 60  # En lugar de 30 segundos
   ```

### Problema 3: "La estructura HTML ha cambiado"

Los sitios web cambian frecuentemente. Si el scraper deja de funcionar:

1. Inspecciona la p√°gina web actual
2. Identifica las nuevas clases CSS o estructura
3. Actualiza los selectores en `_extract_product_info()`

**Ejemplo:**
Si antes era:
```python
title_element = product_element.find('h2', class_='woocommerce-loop-product__title')
```

Y ahora es:
```python
title_element = product_element.find('h2', class_='product-name')
```

### Problema 4: El archivo CSV tiene caracteres raros

Aseg√∫rate de abrir el CSV con codificaci√≥n UTF-8. En Excel:
1. Datos > Desde texto/CSV
2. Selecciona el archivo
3. Origen del archivo: 65001: Unicode (UTF-8)

## ‚ö†Ô∏è Consideraciones √âticas y Legales

### Buenas pr√°cticas de web scraping:

1. **Respeta el robots.txt**: Verifica https://doctorpet.co/robots.txt
2. **No sobrecargues el servidor**: Usa delays apropiados
3. **Respeta los t√©rminos de servicio**: Lee los t√©rminos del sitio web
4. **No hagas scraping de datos personales**: Solo informaci√≥n p√∫blica de productos
5. **Identif√≠cate apropiadamente**: El User-Agent incluye informaci√≥n real del navegador

### Limitaciones t√©cnicas encontradas:

Durante el desarrollo se identificaron los siguientes puntos:

- **DNS/Conectividad**: En algunos entornos, el dominio `doctorpet.co` puede no resolver correctamente debido a configuraciones de red o restricciones DNS
- **Soluci√≥n**: El c√≥digo incluye reintentos autom√°ticos y manejo robusto de errores de conexi√≥n

## üìö Recursos para Aprender M√°s

Si eres nuevo en web scraping, estos recursos te ayudar√°n:

1. **Python Requests**: https://docs.python-requests.org/
2. **BeautifulSoup**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
3. **CSS Selectors**: https://www.w3schools.com/cssref/css_selectors.asp
4. **√âtica en Web Scraping**: https://www.scraperapi.com/blog/web-scraping-laws-ethics/

## ü§ù Contribuciones

Si encuentras un bug o tienes una mejora:

1. Abre un Issue describiendo el problema
2. Si tienes una soluci√≥n, crea un Pull Request
3. Aseg√∫rate de documentar tu cambio siguiendo el patr√≥n Chain of Thought

## üìù Licencia

Este proyecto es de c√≥digo abierto y est√° disponible para fines educativos.

## üë• Autor

Desarrollado como ejemplo educativo para onboarding de desarrolladores junior en web scraping.

---

**¬øPreguntas o problemas?** Abre un issue en el repositorio y te ayudaremos.

