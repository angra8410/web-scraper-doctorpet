# Gu√≠a de Inicio R√°pido - Web Scraper DoctorPet

> üìö **Para desarrolladores junior** - Esta gu√≠a te llevar√° paso a paso desde la instalaci√≥n hasta tu primer scraping exitoso.

## üéØ ¬øQu√© vas a aprender?

Al seguir esta gu√≠a aprender√°s:
1. C√≥mo instalar y configurar el scraper
2. C√≥mo ejecutar tu primer scraping
3. C√≥mo interpretar los resultados
4. C√≥mo solucionar problemas comunes
5. C√≥mo modificar el scraper para tus necesidades

## üì¶ Paso 1: Preparar el Entorno

### 1.1 Verifica que tienes Python instalado

Abre tu terminal (CMD en Windows, Terminal en Mac/Linux) y ejecuta:

```bash
python3 --version
```

Deber√≠as ver algo como: `Python 3.7.0` o superior.

**Si no tienes Python:**
- Windows: Descarga desde https://www.python.org/downloads/
- Mac: `brew install python3`
- Linux: `sudo apt-get install python3 python3-pip`

### 1.2 Clona el repositorio

```bash
# Navega a donde quieres guardar el proyecto
cd ~/Documentos  # o la carpeta que prefieras

# Clona el repositorio
git clone https://github.com/angra8410/web-scraper-doctorpet.git

# Entra al directorio
cd web-scraper-doctorpet
```

### 1.3 Instala las dependencias

```bash
pip install -r requirements.txt
```

Esto instalar√°:
- `requests` - para hacer peticiones HTTP
- `beautifulsoup4` - para analizar HTML
- `lxml` - parser r√°pido para HTML

**Si tienes errores de permisos**, prueba con:
```bash
pip install --user -r requirements.txt
```

## üß™ Paso 2: Prueba que Todo Funciona

Antes de hacer scraping real, vamos a probar con datos de ejemplo:

```bash
python3 test_scraper.py
```

Deber√≠as ver:
```
‚úì Extracci√≥n de productos: 3 productos
‚úì Generaci√≥n de CSV: Exitosa
```

**¬øQu√© acabas de probar?**
- Que todas las bibliotecas se instalaron correctamente
- Que la l√≥gica de extracci√≥n funciona
- Que puedes generar archivos CSV

## üöÄ Paso 3: Tu Primer Scraping Real

¬°Ahora s√≠! Vamos a scrapear datos reales:

```bash
python3 scraper.py
```

### ¬øQu√© est√° pasando?

El scraper:
1. **Se conecta** a https://doctorpet.co/producto-category/alimentos/
2. **Extrae** informaci√≥n de cada producto
3. **Navega** a la siguiente p√°gina autom√°ticamente
4. **Guarda** todo en un archivo CSV

Ver√°s mensajes como:
```
INFO - Haciendo petici√≥n a: https://doctorpet.co/...
INFO - Encontrados 24 productos en esta p√°gina
INFO - ‚úì Extra√≠dos 24 productos de esta p√°gina
```

### ‚è±Ô∏è ¬øPor qu√© es lento?

El scraper espera entre peticiones (2 segundos) para:
- No sobrecargar el servidor
- Evitar que nos bloqueen
- Ser respetuosos con el sitio web

**Esto es NORMAL y NECESARIO** - no lo aceleres sin raz√≥n.

## üìä Paso 4: Encuentra y Abre tu CSV

### Encuentra el archivo

El CSV se guarda con nombre autom√°tico:
```
doctorpet_alimentos_20250930_180515.csv
```

El n√∫mero es la fecha y hora: `A√ëOMESDIA_HORAMINUTOSEGUNDO`

### Abre el archivo

**Con Excel:**
1. Abre Excel
2. Archivo > Abrir
3. Selecciona el CSV
4. Si ves caracteres raros: Datos > Desde texto/CSV > UTF-8

**Con Google Sheets:**
1. Abre Google Sheets
2. Archivo > Importar
3. Arrastra el CSV
4. Importar datos

**Con Python/Pandas:**
```python
import pandas as pd
df = pd.read_csv('doctorpet_alimentos_20250930_180515.csv')
print(df.head())
```

## üîß Paso 5: Personaliza el Scraper

### Cambiar la categor√≠a

Edita `scraper.py` l√≠nea 62:

```python
# Cambiar de alimentos a juguetes
BASE_URL = "https://doctorpet.co/producto-category/juguetes/"
```

### Scrapear solo 2 p√°ginas (para testing)

Edita `scraper.py` l√≠nea 457:

```python
# En lugar de:
productos = scraper.scrape_category()

# Usar:
productos = scraper.scrape_category(max_pages=2)
```

### Ajustar las pausas

Edita `scraper.py` l√≠neas 82-83:

```python
DELAY_BETWEEN_REQUESTS = 3  # M√°s lento, m√°s seguro
DELAY_BETWEEN_PRODUCTS = 1  # Pausa entre cada producto
```

## üêõ Soluci√≥n de Problemas Comunes

### Problema: "No se encontraron productos"

**Causa:** El sitio puede estar ca√≠do o bloqueando el scraper.

**Soluci√≥n:**
1. Abre https://doctorpet.co en tu navegador - ¬øfunciona?
2. Si funciona, ejecuta primero `test_scraper.py` para verificar que el c√≥digo funciona
3. Espera 5 minutos y reintenta

### Problema: "Error de conexi√≥n"

**Causa:** Problemas de red o el sitio no est√° disponible.

**Soluci√≥n:**
```bash
# Verifica tu conexi√≥n
ping google.com

# Verifica que el sitio existe
curl -I https://doctorpet.co
```

### Problema: El CSV est√° vac√≠o o incompleto

**Causa:** El scraper puede haberse interrumpido.

**Soluci√≥n:**
- Revisa los logs en pantalla
- Si se interrumpi√≥ (Ctrl+C), rein√≠cialo
- Los datos se guardan solo AL FINAL

### Problema: Caracteres raros en el CSV (ÔøΩ, ÔøΩ)

**Causa:** Problema de encoding.

**Soluci√≥n:**
- Abre el CSV especificando UTF-8 (ver "Paso 4")
- El archivo est√° correcto, solo necesita abrirse bien

## üìö Pr√≥ximos Pasos

### Aprende m√°s sobre web scraping:
1. Lee los comentarios en `scraper.py` - est√°n pensados para ense√±ar
2. Modifica el scraper para extraer m√°s datos
3. Lee el README.md completo para funcionalidad avanzada

### Proyecto de pr√°ctica:
Intenta modificar el scraper para:
1. Extraer tambi√©n la categor√≠a de cada producto
2. Filtrar solo productos disponibles
3. Ordenar por precio en el CSV

### Recursos recomendados:
- [Documentaci√≥n de BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Gu√≠a de CSS Selectors](https://www.w3schools.com/cssref/css_selectors.asp)
- [Python Requests Tutorial](https://docs.python-requests.org/)

## ‚ùì ¬øNecesitas Ayuda?

1. **Lee primero** el README.md - tiene troubleshooting detallado
2. **Revisa** los logs - el scraper te dice qu√© est√° pasando
3. **Ejecuta** test_scraper.py - verifica que el c√≥digo b√°sico funciona
4. **Abre un issue** en GitHub describiendo tu problema

## ‚úÖ Checklist de √âxito

Marca cada paso que completes:

- [ ] Python instalado y funcionando
- [ ] Repositorio clonado
- [ ] Dependencias instaladas
- [ ] test_scraper.py ejecutado exitosamente
- [ ] scraper.py ejecutado al menos una vez
- [ ] CSV generado y abierto correctamente
- [ ] Entendido por qu√© son importantes los delays
- [ ] Le√≠dos los comentarios en scraper.py

¬°Si marcaste todo, est√°s listo para ser un web scraper! üéâ

---

**¬øTienes sugerencias para mejorar esta gu√≠a?** Abre un Pull Request o Issue.
